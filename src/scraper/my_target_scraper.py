import random
import time
import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
from src.utils.database_handler import DatabaseHandler

class MyTargetScraper:
    def __init__(self, max_workers=1):
        self.base_url = "https://masothue.com/tra-cuu-ma-so-thue-theo-tinh/phuong-xuan-khanh-579"
        self.db_handler = DatabaseHandler(db_path="data-grak.db")
        self.user_agent = UserAgent()

    def get_random_user_agent(self):
        return self.user_agent.random

    def send_request(self, url):
        headers = {"User-Agent": self.get_random_user_agent()}
        try:
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            return r.text
        except requests.RequestException as e:
            print(f"[REQUEST ERROR] {e}")
            return None

    def parse_company_data(self, page_source):
        soup = BeautifulSoup(page_source, "html.parser")
        company_data = []

        rows = soup.select("table.table-hover.table-bordered tbody tr")
        for row in rows:
            cols = [td.get_text(strip=True) for td in row.find_all("td")]
            if len(cols) < 7:
                continue

            company = {
                "name": cols[1].strip(),
                "tax_id": cols[2].strip(),
                "abbreviation": "",  # KhÃ´ng cÃ³ thÃ¬ Ä‘á»ƒ trá»‘ng
                "address": cols[3].strip(),
                "phone": cols[4].strip(),
                "representative": cols[5].strip(),
                "status": cols[6].strip()
            }

            # Loáº¡i bá» kÃ½ tá»± khÃ´ng há»£p lá»‡ (náº¿u cÃ³)
            for key in company:
                company[key] = ''.join(c for c in company[key] if c.isprintable())

            company_data.append(company)

        return company_data


    def save_to_db(self, companies):
        for c in companies:
            self.db_handler.insert_company(c)

    def start(self):
        all_companies = []
        seen_tax_ids = set()
        page = 1

        while True:
            url = f"{self.base_url}?page={page}" if page > 1 else self.base_url
            print(f"â³ Äang cÃ o page {page} â€¦")
            html = self.send_request(url)
            if not html:
                print("KhÃ´ng thá»ƒ truy cáº­p hoáº·c háº¿t page.")
                break

            batch = self.parse_company_data(html)
            if not batch:
                print("â€“ KhÃ´ng tÃ¬m tháº¥y DN trÃªn page nÃ y, dá»«ng.")
                break

            new_batch = [c for c in batch if c["tax_id"] not in seen_tax_ids]
            if not new_batch:
                print("â€“ ToÃ n bá»™ DN Ä‘Ã£ cÃ o trÆ°á»›c Ä‘Ã³, dá»«ng pagination.")
                break

            print(f"â†’ Page {page}: tÃ¬m Ä‘Æ°á»£c {len(batch)} DN, trong Ä‘Ã³ {len(new_batch)} DN má»›i.")
            all_companies.extend(new_batch)
            seen_tax_ids.update(c["tax_id"] for c in new_batch)

            page += 1
            time.sleep(random.uniform(1, 2))

        if all_companies:
            print(f"\nğŸ‰ Tá»•ng cá»™ng cÃ o Ä‘Æ°á»£c {len(all_companies)} DN má»›i. Äang lÆ°u vÃ o DBâ€¦")
            self.save_to_db(all_companies)
            print(f"âœ… ÄÃ£ lÆ°u {len(all_companies)} DN vÃ o database.")
        else:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y DN nÃ o má»›i.")
            
        self.db_handler.close()
